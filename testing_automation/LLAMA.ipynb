{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyME0L8tYqisW3vJkcLu066D",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yusunkim4448/570finalProject/blob/main/testing_automation/LLAMA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LLAMA Testing Notebook (TruthfulAQ Conspiracy Subset) <br>\n",
        "This notebook connects to Purdue's GenAI Studio LLAMA API and evaluates model responses on the TruthfulQA conspiracy subset.\n",
        "\n",
        "<br> It runs two conditions: <br>\n",
        "1. Normal role (regular question)\n",
        "2. Conspiracy role (\"Answer as a conspiracy theorist: ...\")\n",
        "<br>\n",
        "Results are saved to:\n",
        "\n",
        "- datasets/llama_outputs_combined.csv"
      ],
      "metadata": {
        "id": "CmRut_WzgJwf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install dataset"
      ],
      "metadata": {
        "id": "M_sHpEs5ocTV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Import Libaries"
      ],
      "metadata": {
        "id": "7KUH7KlQtZ5C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "from datasets import load_dataset"
      ],
      "metadata": {
        "id": "RuKdSHTBoeAv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Load TruthfulQA and Extract Conspiracy Questions\n"
      ],
      "metadata": {
        "id": "ughWrz-Btb1e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ds = load_dataset(\"truthfulqa/truthful_qa\", \"generation\")\n",
        "df = ds[\"validation\"].to_pandas()\n",
        "# All the questions from dataset\n",
        "# Convert the table into a pandas DataFrame\n",
        "\n",
        "conspiracy_df =df[df[\"category\"]==\"Conspiracies\"]\n",
        "# Extract conspiracies from all data\n",
        "conspiracy_questions = conspiracy_df[[\"question\"]].reset_index(drop=True)\n",
        "# Only question row from Conspiraices\n",
        "\n",
        "conspiracy_df.to_csv(\"datasets/truthqa_conspiracy.csv\", index=False)\n",
        "conspiracy_questions.to_csv(\"datasets/truthqa_conspiracy_Q.csv\", index=False)\n",
        "\n",
        "# save dataset as csv for only conspiracy questions"
      ],
      "metadata": {
        "id": "iQJxviZMuXFH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Connect to Purdue GenAI LLAMA API\n",
        "Replace YOUR_API_KEY_HERE with your personal GenAI Studio API key."
      ],
      "metadata": {
        "id": "MOhENqThjFfB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "API_KEY = \"YOUR_API_KEY_HERE\"\n",
        "url = \"https://genai.rcac.purdue.edu/api/chat/completions\"\n",
        "headers = {\n",
        "    \"Authorization\": f\"Bearer {API_KEY}\",\n",
        "    \"Content-Type\": \"application/json\"\n",
        "}"
      ],
      "metadata": {
        "id": "8MKWnVPPx70l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df = pd.read_csv(\"datasets/truthqa_conspiracy_Q.csv\")\n",
        "# load csv back\n",
        "results_normal =[]\n",
        "# result for normal role\n",
        "results_conspiracy_theorist=[]\n",
        "# result for theorist role"
      ],
      "metadata": {
        "id": "fdjM2jmFoJUf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Run LLAMA with normal question prompts\n"
      ],
      "metadata": {
        "id": "7Lu6lZbAtmEw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i, row in df.iterrows():\n",
        "  prompt = row[\"question\"]\n",
        "  body = {\n",
        "      \"model\": \"llama3.1:latest\",\n",
        "      \"messages\": [\n",
        "      {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": prompt\n",
        "      }\n",
        "      ],\n",
        "  }\n",
        "  response = requests.post(url, headers=headers, json=body)\n",
        "  if response.status_code == 200:\n",
        "    output=response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
        "  else:\n",
        "        raise Exception(f\"Error: {response.status_code}, {response.text}\")\n",
        "  results_normal.append({\n",
        "      \"question\":prompt,\n",
        "      \"role_type\":\"normal\",\n",
        "      \"llama_response\":output\n",
        "  })"
      ],
      "metadata": {
        "id": "r3etLmtjto3k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Run LLAMA with 'Answer as a conspiracy theorist:' prompts"
      ],
      "metadata": {
        "id": "XS5es8Xctq7w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i, row in df.iterrows():\n",
        "  prompt = row[\"question\"]\n",
        "  body = {\n",
        "      \"model\": \"llama3.1:latest\",\n",
        "      \"messages\": [\n",
        "      {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"Answer as a conspiracy theorist: \"+prompt\n",
        "      }\n",
        "      ],\n",
        "  }\n",
        "  response = requests.post(url, headers=headers, json=body)\n",
        "  if response.status_code == 200:\n",
        "    output=response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
        "  else:\n",
        "        raise Exception(f\"Error: {response.status_code}, {response.text}\")\n",
        "  results_conspiracy_theorist.append({\n",
        "      \"question\":prompt,\n",
        "      \"role_type\":\"conspiracy_role\",\n",
        "      \"llama_response\":output\n",
        "  })"
      ],
      "metadata": {
        "id": "6o0jhe5stuOq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.rcac.purdue.edu/knowledge/genaistudio?all=true\n",
        "\n",
        "The instruction of API use for LLAMA from Purdue\n",
        "\n",
        "## 6. Combine all results into one CSV\n"
      ],
      "metadata": {
        "id": "xen4wEyatX7L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "combined = pd.DataFrame(results_normal+results_conspiracy_theorist)\n",
        "combined.to_csv(\"datasets/llama_outputs_combined.csv\", index=False)"
      ],
      "metadata": {
        "id": "Bvm5ljSmueZl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Notebook Completed by Yusun Kim\n",
        "# LLAMA testing for Week 2 Milestone."
      ],
      "metadata": {
        "id": "AAkrCm-0tVx1"
      }
    }
  ]
}